{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a230792-c200-476f-ae2f-1e0ae284288a",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c0dd8a5-a500-4164-8ee9-beab9359778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import r2_score,mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "      LinearRegression,\n",
    "      Ridge,\n",
    "      Lasso\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import (\n",
    "      RandomForestRegressor,\n",
    "      AdaBoostRegressor,\n",
    "      GradientBoostingRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    TransformerMixin\n",
    ")\n",
    "\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "\tOneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    TargetEncoder,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    FunctionTransformer\n",
    ")\n",
    "\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import (\n",
    "    Pipeline,\n",
    "    FeatureUnion\n",
    ")\n",
    "\n",
    "from feature_engine.encoding import(\n",
    "    RareLabelEncoder\n",
    ")\n",
    "\n",
    "from feature_engine.selection import(\n",
    "    SelectBySingleFeaturePerformance,\n",
    "    SmartCorrelatedSelection\n",
    ")\n",
    "\n",
    "import optuna\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471743d-23f3-4a6c-89ac-13100873ed7e",
   "metadata": {},
   "source": [
    "## 2. Display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9485e5f-126b-4262-822a-1435b9aa2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sklearn.set_config(transform_output = \"pandas\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461457b-c365-464e-9d31-e23c65bd697c",
   "metadata": {},
   "source": [
    "## 3. Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9552931-8c15-4729-b299-1b5ba795ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path(r\"F:\\Rishabh\\House-Price-Prediction-MLOps-Project\")\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793e1070-0a66-4f0b-90d0-685150dafebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    file_name = f\"{name}.csv\"\n",
    "    file_path = PROJECT_DIR/DATA_DIR/name/file_name\n",
    "\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24056fe6-1bfd-4c6b-9c86-81427d3ce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88bd475-7372-424a-8f92-19d16268fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = read_data(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed59829e-bd9c-4717-80f8-b6df4229e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b5a402-c0dc-49bd-bf53-74499f43f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = [\"amount\"])\n",
    "y_train = train.amount.copy()\n",
    "\n",
    "X_val = val.drop(columns = [\"amount\"])\n",
    "y_val = val.amount.copy()\n",
    "\n",
    "X_test = test.drop(columns = [\"amount\"])\n",
    "y_test = test.amount.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e80e9c1-ecba-4f3c-95ac-28710f38ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42129, 18)\n",
      "(42129,)\n",
      "(4681, 18)\n",
      "(4681,)\n",
      "(5202, 18)\n",
      "(5202,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b60dee-514b-4553-a27e-94887748008a",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c8e7328-9eaa-471b-ac88-36e5a761443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename the columns before putting back into imputation pipeline\n",
    "def prefix_remover(X, prefixes):\n",
    "\n",
    "    prefix_list = [f\"{prefix}__\" for prefix in prefixes]\n",
    "    new_cols = X.columns\n",
    "    \n",
    "    for prefix in prefix_list:\n",
    "        new_cols = [col.replace(prefix,\"\") if col.startswith(prefix) else col for col in new_cols]\n",
    "\n",
    "    return X.rename(\n",
    "        columns = dict(zip(X.columns,new_cols))\n",
    "    )\n",
    "\n",
    "# imputer to impute bathroom values based on num_bhk\n",
    "class GroupAggregateImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,variable,group_col,estimator,add_indicator = False):\n",
    "        self.variable = variable\n",
    "        self.group_col = group_col\n",
    "        self.estimator = estimator\n",
    "        self.add_indicator = add_indicator\n",
    "\n",
    "    def fit(self,X,y = None):\n",
    "\n",
    "        self.group_medians_ = {}\n",
    "        self.group_modes_ = {}\n",
    "\n",
    "        if self.estimator == \"median\":\n",
    "                self.group_medians_[self.variable] =  X.groupby(self.group_col)[self.variable].median()\n",
    "                \n",
    "        elif self.estimator == \"mode\":\n",
    "                self.group_modes_[self.variable] =  X.groupby(self.group_col)[self.variable].agg(lambda x: x.mode().iloc[0])\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "       X = X.copy()\n",
    "\n",
    "       if self.add_indicator:\n",
    "           X = X.assign(**{\n",
    "               f\"{self.variable}_missingindicator\" : lambda df:(\n",
    "                     np.where(\n",
    "                         df[self.variable].isnull(),\n",
    "                         1,0\n",
    "                     )\n",
    "                 )\n",
    "           })\n",
    "\n",
    "       if self.estimator == \"median\":\n",
    "               mask = X[self.variable].isnull()\n",
    "               X.loc[mask,self.variable] = X.loc[mask,self.group_col].map(self.group_medians_[self.variable])\n",
    "               \n",
    "       elif self.estimator == \"mode\":\n",
    "               mask = X[self.variable].isnull()\n",
    "               X.loc[mask,self.variable] = X.loc[mask,self.group_col].map(self.group_modes_[self.variable]) \n",
    "\n",
    "       \n",
    "       return X\n",
    "\n",
    "# Imputation pipeline\n",
    "\n",
    "furnishing_imputer = ColumnTransformer(transformers = [\n",
    "    (\"furnishing_imputer\",GroupAggregateImputer(variable = \"furnishing\",group_col = \"transaction\",estimator = \"mode\"),[\"furnishing\",\"transaction\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "furnishing_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"furnishing_imputer\",furnishing_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"furnishing_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "floor_num_imputer = ColumnTransformer(transformers = [\n",
    "    (\"floor_num_imputer\",SimpleImputer(strategy = \"median\"),[\"floor_num\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "floor_num_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"floor_num_imputer\",floor_num_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"floor_num_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "num_floors_imputer = ColumnTransformer(transformers = [\n",
    "    (\"num_floors_imputer\",GroupAggregateImputer(variable = \"num_floors\",group_col = \"floor_num\",estimator = \"median\"),[\"num_floors\",\"floor_num\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "num_floors_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"num_floors_imputer\",num_floors_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"num_floors_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "balcony_imputer = ColumnTransformer(transformers = [\n",
    "    (\"balcony_imputer\", GroupAggregateImputer(variable = \"balcony\",group_col = \"num_bhk\",estimator = \"median\", add_indicator = True),[\"balcony\",\"num_bhk\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "balcony_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"balcony_imputer\",balcony_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"balcony_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "ownership_imputer = ColumnTransformer(transformers = [\n",
    "    (\"ownership_imputer\",SimpleImputer(strategy = 'most_frequent',add_indicator = True),[\"ownership\"])\n",
    "], remainder = \"passthrough\")\n",
    "\n",
    "\n",
    "ownership_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"ownership_imputer\",ownership_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"ownership_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "facing_imputer = ColumnTransformer(transformers = [\n",
    "    (\"facing_imputer\",SimpleImputer(strategy = 'constant',fill_value = 'Missing',add_indicator = True),[\"facing\"])\n",
    "], remainder = \"passthrough\")\n",
    "\n",
    "facing_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"facing_imputer\",facing_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"facing_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "overlooking_garden_imputer = ColumnTransformer(transformers = [\n",
    "    (\"overlooking_garden_imputer\",SimpleImputer(strategy = 'constant',fill_value = -1),[\"overlooking_garden\"])\n",
    "], remainder = \"passthrough\")\n",
    "\n",
    "overlooking_garden_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"overlooking_garden_imputer\",overlooking_garden_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"overlooking_garden_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "overlooking_mainroad_imputer = ColumnTransformer(transformers = [\n",
    "    (\"overlooking_mainroad_imputer\",SimpleImputer(strategy = 'constant',fill_value = -1),[\"overlooking_mainroad\"])\n",
    "], remainder = \"passthrough\")\n",
    "\n",
    "overlooking_mainroad_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"overlooking_mainroad_imputer\",overlooking_mainroad_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"overlooking_mainroad_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "overlooking_pool_imputer = ColumnTransformer(transformers = [\n",
    "    (\"overlooking_pool_imputer\",SimpleImputer(strategy = 'constant',fill_value = -1),[\"overlooking_pool\"])\n",
    "], remainder = \"passthrough\")\n",
    "\n",
    "overlooking_pool_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"overlooking_pool_imputer\",overlooking_pool_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"overlooking_pool_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "parking_cover_imputer =  ColumnTransformer(transformers = [\n",
    "    (\"parking_cover_imputer\",SimpleImputer(strategy = 'constant',fill_value = \"No parking\"),[\"parking_cover\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "parking_cover_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"parking_cover_imputer\",parking_cover_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"parking_cover_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "parking_spots_imputer =  ColumnTransformer(transformers = [\n",
    "    (\"parking_spots_imputer\",SimpleImputer(strategy = 'constant',fill_value = 0),[\"parking_spots\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "parking_spots_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"parking_spots_imputer\",parking_spots_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"parking_spots_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "carpet_area_imputer =  ColumnTransformer(transformers = [\n",
    "    (\"carpet_area_imputer\",SimpleImputer(strategy = 'constant',fill_value = -1),[\"carpet_area\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "carpet_area_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"carpet_area_imputer\",carpet_area_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"carpet_area_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "super_area_imputer =  ColumnTransformer(transformers = [\n",
    "    (\"super_area_imputer\",SimpleImputer(strategy = 'constant',fill_value = -1),[\"super_area\"])\n",
    "],remainder = \"passthrough\")\n",
    "\n",
    "super_area_imputation_pipeline = Pipeline(steps = [\n",
    "    (\"super_area_imputer\",super_area_imputer),\n",
    "    (\"prefix_remover\",FunctionTransformer(func = prefix_remover, kw_args = {\"prefixes\" : [\"super_area_imputer\",\"remainder\"]}))\n",
    "])\n",
    "\n",
    "imputation_pipeline = Pipeline(steps = [\n",
    "    (\"furnishing_imputation_pipeline\",furnishing_imputation_pipeline),\n",
    "    (\"floor_num_imputation_pipeline\",floor_num_imputation_pipeline),\n",
    "    (\"num_floors_imputation_pipeline\",num_floors_imputation_pipeline),\n",
    "    (\"balcony_imputation_pipeline\",balcony_imputation_pipeline),\n",
    "    (\"ownership_imputation_pipeline\",ownership_imputation_pipeline),\n",
    "    (\"facing_imputation_pipeline\",facing_imputation_pipeline),\n",
    "    (\"overlooking_garden_imputation_pipeline\",overlooking_garden_imputation_pipeline),\n",
    "    (\"overlooking_mainroad_imputation_pipeline\",overlooking_mainroad_imputation_pipeline),\n",
    "    (\"overlooking_pool_imputation_pipeline\",overlooking_pool_imputation_pipeline),\n",
    "    (\"parking_cover_imputation_pipeline\",parking_cover_imputation_pipeline),\n",
    "    (\"parking_spots_imputation_pipeline\",parking_spots_imputation_pipeline),\n",
    "    (\"carpet_area_imputation_pipeline\",carpet_area_imputation_pipeline),\n",
    "    (\"super_area_imputation_pipeline\",super_area_imputation_pipeline)\n",
    "])\n",
    "\n",
    "# Column transformers\n",
    "\n",
    "transaction_transformer = Pipeline(steps = [\n",
    "    (\"grouper\",RareLabelEncoder(tol = 0.1, n_categories = 2, replace_with = \"Resale\")),\n",
    "    (\"encoder\",OneHotEncoder(sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "def house_size_binner(X):\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "               house_size = lambda df: (\n",
    "                np.select(\n",
    "                [\n",
    "                    df.num_bhk.between(1,3,inclusive = \"left\"),\n",
    "                    df.num_bhk.between(3,4,inclusive = \"left\")\n",
    "                ],\n",
    "                [\"small\",\"normal\"],\n",
    "                default = \"big\"\n",
    "               )\n",
    "            ) \n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "num_bhk_pipe1 = Pipeline(steps = [\n",
    "    (\"scaler\",MinMaxScaler())\n",
    "])\n",
    "\n",
    "num_bhk_pipe2 = Pipeline(steps = [\n",
    "    (\"house_size_binner\",FunctionTransformer(func = house_size_binner)),\n",
    "    (\"encoder\",OrdinalEncoder(categories = [[\"small\",\"normal\",\"big\"]]))\n",
    "])\n",
    "\n",
    "num_bhk_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"num_bhk_pipe1\",num_bhk_pipe1),\n",
    "    (\"num_bhk_pipe2\",num_bhk_pipe2)\n",
    "])\n",
    "\n",
    "def bathroom_num_binner(X):\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "               bathroom_num = lambda df: (\n",
    "                np.select(\n",
    "                [\n",
    "                    df.bathroom.between(1,3,inclusive = \"left\"),\n",
    "                    df.bathroom.between(3,4,inclusive = \"left\")\n",
    "                ],\n",
    "                [\"low\",\"medium\"],\n",
    "                default = \"high\"\n",
    "              )\n",
    "            ) \n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "bathroom_pipe1 = Pipeline(steps = [\n",
    "    (\"scaler\",MinMaxScaler())\n",
    "])\n",
    "\n",
    "bathroom_pipe2 = Pipeline(steps = [\n",
    "    (\"bathroom_num_binner\",FunctionTransformer(func = bathroom_num_binner)),\n",
    "    (\"encoder\",OrdinalEncoder(categories = [[\"low\",\"medium\",\"high\"]]))\n",
    "])\n",
    "\n",
    "bathroom_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"bathroom_pipe1\",bathroom_pipe1),\n",
    "    (\"bathroom_pipe2\",bathroom_pipe2)\n",
    "])\n",
    "\n",
    "def bathroom_num_binner(X):\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "               bathroom_num = lambda df: (\n",
    "                np.select(\n",
    "                [\n",
    "                    df.bathroom.between(1,3,inclusive = \"left\"),\n",
    "                    df.bathroom.between(3,4,inclusive = \"left\")\n",
    "                ],\n",
    "                [\"low\",\"medium\"],\n",
    "                default = \"high\"\n",
    "              )\n",
    "            ) \n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "furnishing_pipe1 = Pipeline(steps = [\n",
    "    (\"encoder\",OrdinalEncoder(categories = [[\"Unfurnished\",\"Semi-Furnished\",\"Furnished\"]])),\n",
    "])\n",
    "\n",
    "furnishing_pipe2 = Pipeline(steps = [\n",
    "    (\"is_unfurnished\",FunctionTransformer(func = lambda x: np.where(x == 'Unfurnished',1,0)))\n",
    "])\n",
    "\n",
    "furnishing_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"furnishing_pipe1\",furnishing_pipe1),\n",
    "    (\"furnishing_pipe2\",furnishing_pipe2)\n",
    "])\n",
    "\n",
    "def floor_height_binner(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "            floor_height = lambda df:(\n",
    "                    np.select(\n",
    "                        [\n",
    "                            (df.floor_num.between(0,3, inclusive = \"left\")),\n",
    "                            (df.floor_num.between(3,6, inclusive = \"left\"))\n",
    "                        ],\n",
    "                        [\"low\",\"medium\"],\n",
    "                        default = \"high\"\n",
    "                    )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "def building_height_binner(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "            building_height = lambda df:(\n",
    "                np.select(\n",
    "                [\n",
    "                    (df.num_floors.between(0,5, inclusive = \"left\")),\n",
    "                    (df.num_floors.between(5,13, inclusive = \"left\"))\n",
    "                ],\n",
    "                [\"short\",\"medium\"],\n",
    "                default = \"tall\"\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "floor_num_pipe1 = Pipeline(steps = [\n",
    "    (\"scaler\",StandardScaler())\n",
    "])\n",
    "\n",
    "floor_num_pipe2 = Pipeline(steps = [\n",
    "    (\"floor_height_binner\",FunctionTransformer(func = floor_height_binner)),\n",
    "    (\"encoder\",OrdinalEncoder(categories = [[\"low\",\"medium\",\"high\"]])),\n",
    "])\n",
    "\n",
    "floor_num_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"floor_num_pipe1\",floor_num_pipe1),\n",
    "    (\"floor_num_pipe2\",floor_num_pipe2)\n",
    "])\n",
    "\n",
    "num_floors_pipe1 = Pipeline(steps = [\n",
    "    (\"scaler\",StandardScaler())\n",
    "])\n",
    "\n",
    "num_floors_pipe2 = Pipeline(steps = [\n",
    "    (\"building_height_binner\",FunctionTransformer(func = building_height_binner)),\n",
    "    (\"encoder\",OrdinalEncoder(categories = [[\"short\",\"medium\",\"tall\"]]))\n",
    "])\n",
    "\n",
    "num_floors_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"num_floors_pipe1\",num_floors_pipe1),\n",
    "    (\"num_floors_pipe2\",num_floors_pipe2)\n",
    "])\n",
    "\n",
    "def city_binner(X):\n",
    "    \n",
    "     columns = X.columns.to_list()\n",
    "\n",
    "     return (\n",
    "        X.assign(\n",
    "            city_tier = lambda df:(\n",
    "                np.where(\n",
    "                    df.location.isin([\"mumbai\",\"gurgaon\",\"new-delhi\"]),\n",
    "                    1,\n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "location_pipe1 = Pipeline(steps = [\n",
    "    (\"target_encoder\", TargetEncoder())\n",
    "])\n",
    "\n",
    "location_pipe2 = Pipeline(steps = [\n",
    "    (\"city_binner\",FunctionTransformer(func = city_binner))\n",
    "])\n",
    "\n",
    "location_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"location_pipe1\",location_pipe1),\n",
    "    (\"location_pipe2\",location_pipe2)\n",
    "])\n",
    "\n",
    "def price_binner(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "               price_range = lambda df:(\n",
    "                np.select(\n",
    "                    [\n",
    "                        df.price.between(0,4000,inclusive = \"left\"),\n",
    "                        df.price.between(4000,6000,inclusive = \"left\")\n",
    "                    ],\n",
    "                    [\"low\",\"medium\"],\n",
    "                    default = \"high\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "balcony_transformer = Pipeline(steps = [\n",
    "    (\"nearest_integer\",FunctionTransformer(func = lambda x : np.round(x))),\n",
    "    (\"scaler\",MinMaxScaler())\n",
    "])\n",
    "\n",
    "ownership_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "missingindicator_ownership_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(drop = 'first',sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "def direction_binner(X):\n",
    "\n",
    "     columns = X.columns.to_list()\n",
    "\n",
    "     return (\n",
    "        X.assign(\n",
    "            direction_tier = lambda df:(\n",
    "                np.where(\n",
    "                    df.facing.isin([\"North - East\",\"North - West\"]),\n",
    "                    1,\n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "     )\n",
    "\n",
    "facing_pipe1 = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "facing_pipe2 = Pipeline(steps = [\n",
    "    (\"direction_binner\",FunctionTransformer(func = direction_binner))\n",
    "])\n",
    "\n",
    "facing_transformer = FeatureUnion(transformer_list = [\n",
    "    (\"facing_pipe1\",facing_pipe1),\n",
    "    (\"facing_pipe2\",facing_pipe2)\n",
    "])\n",
    "\n",
    "missingindicator_facing_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(drop = 'first',sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "overlooking_garden_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "overlooking_mainroad_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(categories=[[ -1, 0, 1 ]], drop=[-1], sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "overlooking_pool_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(categories=[[ -1, 0, 1 ]], drop=[-1], sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "parking_cover_transformer = Pipeline(steps = [\n",
    "    (\"encoder\",OneHotEncoder(sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "def has_parking(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        X.assign(\n",
    "            has_parking = lambda df:(\n",
    "                np.select(\n",
    "                            [\n",
    "                                df.parking_spots.eq(0),\n",
    "                                df.parking_spots.eq(1)\n",
    "                            ],\n",
    "                            [\"no parking\",\"single\"],\n",
    "                            default = \"multiple\"\n",
    "                        )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "     )\n",
    "\n",
    "parking_spots_transformer = Pipeline(steps = [\n",
    "    (\"has_parking\",FunctionTransformer(func = has_parking)),\n",
    "    (\"encoder\",OneHotEncoder(categories = [[\"multiple\",\"single\",\"no parking\"]],drop = [[\"no parking\"]],sparse_output = False,handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "def effective_area(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return(\n",
    "        X\n",
    "        .assign(\n",
    "            effective_area = lambda df:(\n",
    "                np.where(\n",
    "                    df.carpet_area.eq(-1),\n",
    "                    df.super_area,\n",
    "                    df.carpet_area\n",
    "                )\n",
    "            ),\n",
    "            carpet_areamissing = lambda df:(\n",
    "                np.where(\n",
    "                    df.carpet_area.eq(-1),\n",
    "                    1,\n",
    "                    0\n",
    "                )\n",
    "            ),\n",
    "            super_areamissing = lambda df:(\n",
    "                np.where(\n",
    "                    df.super_area.eq(-1),\n",
    "                    1,\n",
    "                    0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "scaler_pipeline = Pipeline(steps = [\n",
    "    (\"log_transformer\",FunctionTransformer(func = lambda x: np.log(x))),\n",
    "    (\"scaler\",StandardScaler())\n",
    "])\n",
    "\n",
    "area_transformer = Pipeline(steps = [\n",
    "    (\"effective_area\",FunctionTransformer(func = effective_area)),\n",
    "    (\"scaler_pipeline\",ColumnTransformer(transformers = [\n",
    "        (\"scaler_pipeline\",scaler_pipeline,[\"effective_area\"])\n",
    "    ],remainder = \"passthrough\")),\n",
    "])\n",
    "\n",
    "def area_per_room(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return(\n",
    "        X\n",
    "        .assign(\n",
    "            area_per_room = lambda df:(\n",
    "                np.where(\n",
    "                    df.carpet_area.eq(-1),\n",
    "                    df.super_area/df.num_bhk,\n",
    "                    df.carpet_area/df.num_bhk\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "scaler_pipeline = Pipeline(steps = [\n",
    "    (\"log_transformer\",FunctionTransformer(func = lambda x: np.log(x))),\n",
    "    (\"scaler\",StandardScaler())\n",
    "])\n",
    "\n",
    "area_per_room_transformer = Pipeline(steps = [\n",
    "    (\"area_per_room\",FunctionTransformer(func = area_per_room)),\n",
    "    (\"scaler_pipeline\",ColumnTransformer(transformers = [\n",
    "        (\"scaler_pipeline\",scaler_pipeline,[\"area_per_room\"])\n",
    "    ],remainder = \"passthrough\"))\n",
    "])\n",
    "\n",
    "def balcony_per_room(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return(\n",
    "        X\n",
    "        .assign(\n",
    "            balcony_per_room = lambda df:(\n",
    "                df.balcony/df.num_bhk\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "def bathroom_per_room(X):\n",
    "\n",
    "    columns = X.columns.to_list()\n",
    "\n",
    "    return(\n",
    "        X\n",
    "        .assign(\n",
    "            bathroom_per_room = lambda df:(\n",
    "                df.bathroom/df.num_bhk\n",
    "            )\n",
    "        )\n",
    "        .drop(columns = columns)\n",
    "    )\n",
    "\n",
    "balcony_per_room_transformer = Pipeline(steps = [\n",
    "    (\"balcony_per_room\",FunctionTransformer(func = balcony_per_room))\n",
    "])\n",
    "\n",
    "bathroom_per_room_transformer = Pipeline(steps = [\n",
    "    (\"bathroom_per_room\",FunctionTransformer(func = bathroom_per_room))\n",
    "])\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers = [\n",
    "    (\"transaction_transformer\",transaction_transformer,[\"transaction\"]),\n",
    "    (\"num_bhk_transformer\",num_bhk_transformer,[\"num_bhk\"]),\n",
    "    (\"bathroom_transformer\",bathroom_transformer,[\"bathroom\"]),\n",
    "    (\"furnishing_transformer\",furnishing_transformer,[\"furnishing\"]),\n",
    "    (\"floor_num_transformer\",floor_num_transformer,[\"floor_num\"]),\n",
    "    (\"num_floors_transformer\",num_floors_transformer,[\"num_floors\"]),\n",
    "    (\"location_transformer\",location_transformer,[\"location\"]),\n",
    "    (\"balcony_transformer\",balcony_transformer,[\"balcony\"]),\n",
    "    (\"ownership_transformer\",ownership_transformer,[\"ownership\"]),\n",
    "    (\"missingindicator_ownership_transformer\",missingindicator_ownership_transformer,[\"missingindicator_ownership\"]),\n",
    "    (\"facing_transformer\",facing_transformer,[\"facing\"]),\n",
    "    (\"missingindicator_facing_transformer\",missingindicator_ownership_transformer,[\"missingindicator_facing\"]),\n",
    "    (\"overlooking_garden_transformer\",overlooking_garden_transformer,[\"overlooking_garden\"]),\n",
    "    (\"overlooking_mainroad_transformer\",overlooking_mainroad_transformer,[\"overlooking_mainroad\"]),\n",
    "    (\"overlooking_pool_transformer\",overlooking_pool_transformer,[\"overlooking_pool\"]),\n",
    "    (\"parking_cover_transformer\",parking_cover_transformer,[\"parking_cover\"]),\n",
    "    (\"parking_spots_transformer\",parking_spots_transformer,[\"parking_spots\"]),\n",
    "    (\"area_transformer\",area_transformer,[\"carpet_area\",\"super_area\"]),\n",
    "    (\"area_per_room_transformer\",area_per_room_transformer,[\"carpet_area\",\"super_area\",\"num_bhk\"]),\n",
    "    (\"balcony_per_room_transformer\",balcony_per_room_transformer,[\"balcony\",\"num_bhk\"]),\n",
    "    (\"bathroom_per_room_transformer\",bathroom_per_room_transformer,[\"bathroom\",\"num_bhk\"])\n",
    "],remainder = 'passthrough')\n",
    "\n",
    "# Final preprocessor\n",
    "\n",
    "feature_preprocessor = Pipeline(steps = [\n",
    "    (\"imputation_pipeline\",imputation_pipeline),\n",
    "    (\"column_transformer\",column_transformer)\n",
    "])\n",
    "\n",
    "# Correlated feature selector\n",
    "drop_correlated = SmartCorrelatedSelection(estimator = RandomForestRegressor(), scoring = 'r2')\n",
    "\n",
    "# Feature selector\n",
    "feature_selector = SelectBySingleFeaturePerformance(estimator = RandomForestRegressor(),scoring = 'r2',threshold = 0.01)\n",
    "\n",
    "# Feature engineering pipeline\n",
    "feature_engineering_pipeline = Pipeline(steps = [\n",
    "    (\"feature_preprocessor\",feature_preprocessor),\n",
    "    (\"drop_correlated\",drop_correlated),\n",
    "    # (\"feature_selector\",feature_selector)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0668e8dc-80a5-4bee-afd2-28cbbb3653b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = feature_engineering_pipeline.fit_transform(X_train,y_train)\n",
    "X_val_preprocessed = feature_engineering_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69753df-6579-4daa-8448-225d73c610c8",
   "metadata": {},
   "source": [
    "## 5. Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ec4175e-2c38-4f4a-b7d9-eadf28cf29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "        (\"model\", TransformedTargetRegressor(\n",
    "            regressor=RandomForestRegressor(),\n",
    "            transformer=FunctionTransformer(func=np.sqrt,inverse_func= lambda x:x**2)\n",
    "        ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f02c79-d16d-4242-a44a-8cf2ef2e3739",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2ba1e31-317d-4fe1-a386-fc887a95d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    regressor_name = trial.suggest_categorical(\"regressor\", [\n",
    "        \"LinearRegression\",\n",
    "        \"RidgeRegression\",\n",
    "        \"LassoRegression\",\n",
    "        \"KNeighborsRegressor\",\n",
    "        \"RandomForestRegressor\",\n",
    "        \"AdaBoostRegressor\",\n",
    "        \"GradientBoostingRegressor\",\n",
    "        \"XGBRegressor\",\n",
    "    ])\n",
    "\n",
    "    if regressor_name == \"LinearRegression\":\n",
    "        fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        regressor = LinearRegression(\n",
    "            fit_intercept=fit_intercept,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"RidgeRegression\":\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.0001, 10.0, log=True)\n",
    "        fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        regressor = Ridge(\n",
    "            alpha=alpha,\n",
    "            fit_intercept=fit_intercept\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"LassoRegression\":\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n",
    "        fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        regressor = Lasso(\n",
    "            alpha=alpha,\n",
    "            fit_intercept=fit_intercept\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"KNeighborsRegressor\":\n",
    "        n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 30)\n",
    "        weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "        p = trial.suggest_int(\"p\", 1, 2)  \n",
    "        regressor = KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            p=p,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"RandomForestRegressor\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        regressor = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_features=max_features,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"AdaBoostRegressor\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 2.0, log=True)\n",
    "        loss = trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"])\n",
    "        regressor = AdaBoostRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"GradientBoostingRegressor\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 2.0, log=True)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        regressor = GradientBoostingRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=subsample,\n",
    "            learning_rate=learning_rate,\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features\n",
    "        )\n",
    "\n",
    "    elif regressor_name == \"XGBRegressor\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "        regressor = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    target_transformer_name = trial.suggest_categorical(\n",
    "        \"target_transformer\",\n",
    "        [\"log\",\"cbrt\",\"sqrt\",\"same\"]\n",
    "    )\n",
    "\n",
    "    if target_transformer_name == \"log\":\n",
    "        transformer = FunctionTransformer(\n",
    "            func = np.log,\n",
    "            inverse_func = np.exp\n",
    "        )\n",
    "        \n",
    "    elif target_transformer_name == \"cbrt\":\n",
    "        transformer = FunctionTransformer(\n",
    "            func = np.cbrt,\n",
    "            inverse_func = lambda x:x**3\n",
    "        )\n",
    "\n",
    "    elif target_transformer_name == \"sqrt\":\n",
    "        transformer = FunctionTransformer(\n",
    "            func = np.sqrt,\n",
    "            inverse_func = lambda x:x**2\n",
    "        )\n",
    "\n",
    "    elif target_transformer_name == \"same\":\n",
    "        transformer = FunctionTransformer(\n",
    "            func = lambda x:x,\n",
    "            inverse_func = lambda x:x\n",
    "        )\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        (\"model\", TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=transformer\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # model_pipeline = Pipeline(steps = [\n",
    "    #     (\"model\",regressor)\n",
    "    # ])\n",
    "\n",
    "    score = 0\n",
    "    for i in range(3):\n",
    "        model_pipeline.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "        train_mae = mean_absolute_error(y_train,model_pipeline.predict(X_train_preprocessed))\n",
    "        val_mae = mean_absolute_error(y_val,model_pipeline.predict(X_val_preprocessed))\n",
    "        \n",
    "        train_score = r2_score(y_train,model_pipeline.predict(X_train_preprocessed))\n",
    "        val_score = r2_score(y_val,model_pipeline.predict(X_val_preprocessed))\n",
    "        \n",
    "        overfit_penalty = abs(train_score-val_score)\n",
    "        score += val_score - overfit_penalty\n",
    "\n",
    "    return score/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23afba02-42f7-4da9-b058-8d86037ae431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 10:27:59,917] A new study created in memory with name: no-name-8886360c-26d7-432b-99af-17e0c2c9693a\n",
      "[I 2025-08-11 10:29:03,322] Trial 0 finished with value: 0.12973827711944844 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 159, 'subsample': 0.9098488406861582, 'learning_rate': 0.0011436874909193566, 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2', 'target_transformer': 'sqrt'}. Best is trial 0 with value: 0.12973827711944844.\n",
      "[I 2025-08-11 10:33:47,578] Trial 1 finished with value: 0.6893786870304038 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 147, 'subsample': 0.9163069141465758, 'learning_rate': 0.009881338990388271, 'min_samples_split': 7, 'max_depth': 7, 'max_features': None, 'target_transformer': 'sqrt'}. Best is trial 1 with value: 0.6893786870304038.\n",
      "[I 2025-08-11 10:34:39,638] Trial 2 finished with value: 0.6459680512085285 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.0005502663690235641, 'fit_intercept': False, 'target_transformer': 'same'}. Best is trial 1 with value: 0.6893786870304038.\n",
      "[I 2025-08-11 10:34:46,416] Trial 3 finished with value: 0.7692836197503187 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 120, 'learning_rate': 0.030675710898345032, 'max_depth': 5, 'subsample': 0.6547786066995698, 'colsample_bytree': 0.5297701323321043, 'target_transformer': 'sqrt'}. Best is trial 3 with value: 0.7692836197503187.\n",
      "[I 2025-08-11 10:35:33,760] Trial 4 finished with value: 0.6057704237784568 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 144, 'subsample': 0.8896488798859084, 'learning_rate': 0.008042961735339714, 'min_samples_split': 9, 'max_depth': 6, 'max_features': 'log2', 'target_transformer': 'same'}. Best is trial 3 with value: 0.7692836197503187.\n",
      "[I 2025-08-11 10:36:11,565] Trial 5 finished with value: 0.7103044051773016 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 297, 'max_depth': 12, 'min_samples_split': 6, 'max_features': 'sqrt', 'target_transformer': 'log'}. Best is trial 3 with value: 0.7692836197503187.\n",
      "[I 2025-08-11 10:36:12,307] Trial 6 finished with value: 0.6502017166087632 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': False, 'target_transformer': 'same'}. Best is trial 3 with value: 0.7692836197503187.\n",
      "[I 2025-08-11 10:36:24,033] Trial 7 finished with value: 0.7623093462106341 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 178, 'learning_rate': 0.015607847065115098, 'max_depth': 7, 'subsample': 0.8202491620377106, 'colsample_bytree': 0.5479173979840686, 'target_transformer': 'same'}. Best is trial 3 with value: 0.7692836197503187.\n",
      "[I 2025-08-11 10:36:28,219] Trial 8 finished with value: 0.7783118961020664 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 72, 'learning_rate': 0.1820865535413363, 'max_depth': 6, 'subsample': 0.9327881140487317, 'colsample_bytree': 0.6596498294134776, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:36:28,731] Trial 9 finished with value: 0.6879790382101287 and parameters: {'regressor': 'RidgeRegression', 'alpha': 0.8689348680449663, 'fit_intercept': True, 'target_transformer': 'sqrt'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:46:45,212] Trial 10 finished with value: 0.6536125281199742 and parameters: {'regressor': 'KNeighborsRegressor', 'n_neighbors': 22, 'weights': 'uniform', 'p': 1, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:46:48,185] Trial 11 finished with value: 0.7569420621630361 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 52, 'learning_rate': 0.2743237321036024, 'max_depth': 2, 'subsample': 0.6021508478863528, 'colsample_bytree': 0.5972250663053691, 'target_transformer': 'cbrt'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:46:51,556] Trial 12 finished with value: 0.7768673580823982 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 69, 'learning_rate': 0.19811275519320665, 'max_depth': 4, 'subsample': 0.6600579191369144, 'colsample_bytree': 0.796345470170952, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:47:56,318] Trial 13 finished with value: 0.6356426825248778 and parameters: {'regressor': 'AdaBoostRegressor', 'n_estimators': 50, 'learning_rate': 0.5834798297773718, 'loss': 'square', 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:48:00,409] Trial 14 finished with value: 0.7684640270083634 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 92, 'learning_rate': 0.08608339884481524, 'max_depth': 4, 'subsample': 0.7116611021178451, 'colsample_bytree': 0.8495106697369893, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:48:07,492] Trial 15 finished with value: 0.772755519050647 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 208, 'learning_rate': 0.09269081137918389, 'max_depth': 3, 'subsample': 0.539740590408127, 'colsample_bytree': 0.7784423464763236, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:48:08,304] Trial 16 finished with value: 0.6944807968665189 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': True, 'target_transformer': 'cbrt'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:50:23,205] Trial 17 finished with value: 0.41671326308670126 and parameters: {'regressor': 'KNeighborsRegressor', 'n_neighbors': 4, 'weights': 'distance', 'p': 2, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:50:23,736] Trial 18 finished with value: 0.7039684849141447 and parameters: {'regressor': 'RidgeRegression', 'alpha': 8.241936833810668, 'fit_intercept': True, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:53:34,787] Trial 19 finished with value: 0.7046508437239716 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.00012434355440047308, 'fit_intercept': False, 'target_transformer': 'log'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:53:55,577] Trial 20 finished with value: 0.6713719057784054 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 86, 'max_depth': 19, 'min_samples_split': 2, 'max_features': 'sqrt', 'target_transformer': 'cbrt'}. Best is trial 8 with value: 0.7783118961020664.\n",
      "[I 2025-08-11 10:54:03,598] Trial 21 finished with value: 0.7819019431109222 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 227, 'learning_rate': 0.10253959602087749, 'max_depth': 3, 'subsample': 0.5171304737623511, 'colsample_bytree': 0.7556763105903261, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:54:13,060] Trial 22 finished with value: 0.7775896451679586 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 238, 'learning_rate': 0.1126004390930682, 'max_depth': 4, 'subsample': 0.5013415157128394, 'colsample_bytree': 0.6903984028280836, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:54:19,935] Trial 23 finished with value: 0.7442087326784148 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 234, 'learning_rate': 0.059730738582064645, 'max_depth': 2, 'subsample': 0.5238582049669419, 'colsample_bytree': 0.6866434266028163, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:58:55,110] Trial 24 finished with value: 0.6234811636327798 and parameters: {'regressor': 'AdaBoostRegressor', 'n_estimators': 250, 'learning_rate': 1.8446223231667387, 'loss': 'exponential', 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:59:04,949] Trial 25 finished with value: 0.7732816889823934 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 261, 'learning_rate': 0.0415349294421046, 'max_depth': 4, 'subsample': 0.9850007893345148, 'colsample_bytree': 0.680442315385251, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:59:13,685] Trial 26 finished with value: 0.7816735795967302 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 202, 'learning_rate': 0.12426253152817714, 'max_depth': 5, 'subsample': 0.5064722365382265, 'colsample_bytree': 0.9575189645635347, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:59:24,112] Trial 27 finished with value: 0.7281970104088838 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 200, 'learning_rate': 0.1743949355065546, 'max_depth': 6, 'subsample': 0.7686338968978612, 'colsample_bytree': 0.9846060788611721, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:59:45,975] Trial 28 finished with value: 0.7450607576475742 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 202, 'learning_rate': 0.028634681222758526, 'max_depth': 9, 'subsample': 0.5812792446385642, 'colsample_bytree': 0.9554707483684923, 'target_transformer': 'cbrt'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 10:59:46,719] Trial 29 finished with value: 0.6880520437514569 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': False, 'target_transformer': 'sqrt'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:01:10,839] Trial 30 finished with value: 0.7494665042128785 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 180, 'max_depth': 11, 'min_samples_split': 10, 'max_features': None, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:01:20,679] Trial 31 finished with value: 0.7722189100295401 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 229, 'learning_rate': 0.10140539552219842, 'max_depth': 5, 'subsample': 0.5019834957070326, 'colsample_bytree': 0.6671744499633776, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:02:05,311] Trial 32 finished with value: 0.7343997926874736 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 267, 'subsample': 0.5702107010531147, 'learning_rate': 0.4711111657580677, 'min_samples_split': 2, 'max_depth': 3, 'max_features': 'sqrt', 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:02:15,606] Trial 33 finished with value: 0.7738027955819103 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 224, 'learning_rate': 0.12380816723295222, 'max_depth': 5, 'subsample': 0.5029121874642803, 'colsample_bytree': 0.8662089278375802, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:02:17,313] Trial 34 finished with value: 0.5790876259123592 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.014569489071803419, 'fit_intercept': True, 'target_transformer': 'sqrt'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:02:26,249] Trial 35 finished with value: 0.7714968723175916 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 286, 'learning_rate': 0.060890553003161395, 'max_depth': 3, 'subsample': 0.6199387680410479, 'colsample_bytree': 0.7143719388704316, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:06:01,185] Trial 36 finished with value: 0.651053926926474 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 245, 'subsample': 0.5524468620832859, 'learning_rate': 0.4068013120474838, 'min_samples_split': 6, 'max_depth': 5, 'max_features': None, 'target_transformer': 'same'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:06:08,939] Trial 37 finished with value: 0.7734438096202068 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 211, 'learning_rate': 0.14443602297637642, 'max_depth': 4, 'subsample': 0.8088645173955452, 'colsample_bytree': 0.6271924284447904, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:06:09,560] Trial 38 finished with value: 0.6502016345816639 and parameters: {'regressor': 'RidgeRegression', 'alpha': 0.025960368471833896, 'fit_intercept': True, 'target_transformer': 'same'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:08:29,559] Trial 39 finished with value: 0.7028309289047513 and parameters: {'regressor': 'AdaBoostRegressor', 'n_estimators': 117, 'learning_rate': 0.8140482735311015, 'loss': 'linear', 'target_transformer': 'sqrt'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:19:04,193] Trial 40 finished with value: 0.6532768039851705 and parameters: {'regressor': 'KNeighborsRegressor', 'n_neighbors': 29, 'weights': 'uniform', 'p': 1, 'target_transformer': 'log'}. Best is trial 21 with value: 0.7819019431109222.\n",
      "[I 2025-08-11 11:19:08,373] Trial 41 finished with value: 0.7883043894761069 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 80, 'learning_rate': 0.1872161619586628, 'max_depth': 4, 'subsample': 0.6713624531466091, 'colsample_bytree': 0.7844286856077352, 'target_transformer': 'log'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:12,364] Trial 42 finished with value: 0.7341473281163472 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 94, 'learning_rate': 0.05957494027687505, 'max_depth': 3, 'subsample': 0.6800401529036755, 'colsample_bytree': 0.732240390310349, 'target_transformer': 'log'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:18,670] Trial 43 finished with value: 0.7818963868915031 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 164, 'learning_rate': 0.15045770457425772, 'max_depth': 4, 'subsample': 0.5455153388287234, 'colsample_bytree': 0.9090607212183425, 'target_transformer': 'log'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:25,580] Trial 44 finished with value: 0.760393862373434 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 120, 'learning_rate': 0.1814505075917636, 'max_depth': 6, 'subsample': 0.6152362346182937, 'colsample_bytree': 0.9116898136126261, 'target_transformer': 'log'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:32,846] Trial 45 finished with value: 0.7825384859457242 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 158, 'learning_rate': 0.07252229697647326, 'max_depth': 5, 'subsample': 0.5560257426473132, 'colsample_bytree': 0.9130366060962073, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:40,593] Trial 46 finished with value: 0.46763698276480553 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 158, 'learning_rate': 0.0035399174931364354, 'max_depth': 5, 'subsample': 0.5582261234833787, 'colsample_bytree': 0.927898103109297, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:52,119] Trial 47 finished with value: 0.6367595841166586 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.0016192185594548466, 'fit_intercept': False, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:59,228] Trial 48 finished with value: 0.7817328855059635 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 189, 'learning_rate': 0.06943845747224808, 'max_depth': 4, 'subsample': 0.5850761932461296, 'colsample_bytree': 0.8644730445525936, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:19:59,980] Trial 49 finished with value: 0.6501992187436587 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': True, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:20:14,911] Trial 50 finished with value: 0.7378387037308011 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 159, 'max_depth': 9, 'min_samples_split': 8, 'max_features': 'log2', 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:20:22,027] Trial 51 finished with value: 0.7836628154071 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 189, 'learning_rate': 0.07733016478179065, 'max_depth': 4, 'subsample': 0.537436757582329, 'colsample_bytree': 0.8703763796706961, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:20:27,615] Trial 52 finished with value: 0.7841453568254192 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 137, 'learning_rate': 0.07088639178262826, 'max_depth': 4, 'subsample': 0.5844621584953436, 'colsample_bytree': 0.857792027812866, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:20:32,808] Trial 53 finished with value: 0.767335013080649 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 133, 'learning_rate': 0.04128929066703449, 'max_depth': 3, 'subsample': 0.5450666675323617, 'colsample_bytree': 0.8108315297217757, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:20:39,315] Trial 54 finished with value: 0.7869326770329351 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 153, 'learning_rate': 0.04052671231656445, 'max_depth': 4, 'subsample': 0.612672334940541, 'colsample_bytree': 0.8993521622314201, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:22:53,496] Trial 55 finished with value: 0.4038282759271308 and parameters: {'regressor': 'KNeighborsRegressor', 'n_neighbors': 3, 'weights': 'distance', 'p': 2, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:22:53,999] Trial 56 finished with value: 0.6501929750211402 and parameters: {'regressor': 'RidgeRegression', 'alpha': 0.26661285834352333, 'fit_intercept': False, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:22:59,066] Trial 57 finished with value: 0.7686530770737264 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 143, 'learning_rate': 0.03862779182236035, 'max_depth': 3, 'subsample': 0.6322830622773756, 'colsample_bytree': 0.8327504459717114, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:23:19,755] Trial 58 finished with value: 0.6369131659100293 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 170, 'subsample': 0.5907091366675444, 'learning_rate': 0.02080765744478286, 'min_samples_split': 4, 'max_depth': 2, 'max_features': 'log2', 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:23:26,237] Trial 59 finished with value: 0.16799567964267614 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 133, 'learning_rate': 0.0011072615559399696, 'max_depth': 5, 'subsample': 0.6435172539532203, 'colsample_bytree': 0.7676588660998438, 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:28:15,619] Trial 60 finished with value: 0.6498257277056748 and parameters: {'regressor': 'AdaBoostRegressor', 'n_estimators': 186, 'learning_rate': 0.012494904268145009, 'loss': 'square', 'target_transformer': 'same'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:28:24,015] Trial 61 finished with value: 0.7858166768287321 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 167, 'learning_rate': 0.07175734344612567, 'max_depth': 4, 'subsample': 0.5544355258229107, 'colsample_bytree': 0.8997052210199478, 'target_transformer': 'cbrt'}. Best is trial 41 with value: 0.7883043894761069.\n",
      "[I 2025-08-11 11:28:31,278] Trial 62 finished with value: 0.7891217334721862 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 141, 'learning_rate': 0.074563264253282, 'max_depth': 4, 'subsample': 0.5732166868781123, 'colsample_bytree': 0.8897608119721337, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:28:38,720] Trial 63 finished with value: 0.7869279778347589 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 153, 'learning_rate': 0.07686716514066214, 'max_depth': 4, 'subsample': 0.5744567527960776, 'colsample_bytree': 0.8953682875706692, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:28:44,955] Trial 64 finished with value: 0.774349049628897 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 111, 'learning_rate': 0.04630190102600853, 'max_depth': 4, 'subsample': 0.6774912708784867, 'colsample_bytree': 0.8795412897957965, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:28:52,913] Trial 65 finished with value: 0.7551148862632263 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 147, 'learning_rate': 0.024729208738486136, 'max_depth': 4, 'subsample': 0.6079342467049987, 'colsample_bytree': 0.8896748497629012, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:29:01,550] Trial 66 finished with value: 0.7890010915070832 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 134, 'learning_rate': 0.07717170221092205, 'max_depth': 4, 'subsample': 0.5806230336295015, 'colsample_bytree': 0.8273514200677088, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:29:09,772] Trial 67 finished with value: 0.7815467773989772 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 134, 'learning_rate': 0.051524408031253785, 'max_depth': 4, 'subsample': 0.5919259891665021, 'colsample_bytree': 0.8269731378121897, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:29:17,292] Trial 68 finished with value: 0.749440280716201 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 150, 'learning_rate': 0.03428750029530027, 'max_depth': 3, 'subsample': 0.7297114996630838, 'colsample_bytree': 0.8424848472942709, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:29:18,157] Trial 69 finished with value: 0.6944807968665189 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': True, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:29:54,028] Trial 70 finished with value: 0.7591357845934904 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 99, 'max_depth': 7, 'min_samples_split': 4, 'max_features': None, 'target_transformer': 'cbrt'}. Best is trial 62 with value: 0.7891217334721862.\n",
      "[I 2025-08-11 11:30:02,685] Trial 71 finished with value: 0.7903919143053822 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 174, 'learning_rate': 0.08556938450117299, 'max_depth': 4, 'subsample': 0.5697741946697343, 'colsample_bytree': 0.8939184523786483, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:30:11,772] Trial 72 finished with value: 0.7886531868794967 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 171, 'learning_rate': 0.0873005370654975, 'max_depth': 4, 'subsample': 0.5688477665717074, 'colsample_bytree': 0.9387076451330247, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:30:21,477] Trial 73 finished with value: 0.7900482672125032 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 170, 'learning_rate': 0.08784865691292613, 'max_depth': 4, 'subsample': 0.6157945488656793, 'colsample_bytree': 0.9428802716543558, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:31:09,931] Trial 74 finished with value: 0.6180977531253862 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.0027259603565928606, 'fit_intercept': False, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:31:18,902] Trial 75 finished with value: 0.7785348612307126 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 173, 'learning_rate': 0.09919207528434128, 'max_depth': 5, 'subsample': 0.6529420146147481, 'colsample_bytree': 0.9391599826150455, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:31:23,301] Trial 76 finished with value: 0.7827937713442145 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 74, 'learning_rate': 0.08870570199869021, 'max_depth': 4, 'subsample': 0.6303797143843488, 'colsample_bytree': 0.9727610487898051, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:31:23,851] Trial 77 finished with value: 0.6932162897194942 and parameters: {'regressor': 'RidgeRegression', 'alpha': 9.475061107032046, 'fit_intercept': False, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:31:32,011] Trial 78 finished with value: 0.5063091343156825 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 180, 'learning_rate': 0.006274407792743866, 'max_depth': 3, 'subsample': 0.606428521763045, 'colsample_bytree': 0.9376914304081861, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:33:59,720] Trial 79 finished with value: 0.6727294742090729 and parameters: {'regressor': 'KNeighborsRegressor', 'n_neighbors': 13, 'weights': 'uniform', 'p': 2, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:34:07,025] Trial 80 finished with value: 0.7888202117210215 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 153, 'learning_rate': 0.12535556398888056, 'max_depth': 4, 'subsample': 0.685651408295508, 'colsample_bytree': 0.8910645812764812, 'target_transformer': 'cbrt'}. Best is trial 71 with value: 0.7903919143053822.\n",
      "[I 2025-08-11 11:34:13,889] Trial 81 finished with value: 0.7957138059273478 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 153, 'learning_rate': 0.12744409833293713, 'max_depth': 4, 'subsample': 0.687906288849064, 'colsample_bytree': 0.9966478096303656, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:34:20,975] Trial 82 finished with value: 0.7874029418538787 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 145, 'learning_rate': 0.14001572439260662, 'max_depth': 4, 'subsample': 0.7007027413730752, 'colsample_bytree': 0.9988756917155209, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:34:26,298] Trial 83 finished with value: 0.7891624590732335 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 126, 'learning_rate': 0.12585303020821484, 'max_depth': 3, 'subsample': 0.6920737413482643, 'colsample_bytree': 0.9984543375023252, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:34:31,274] Trial 84 finished with value: 0.7718479493499707 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 124, 'learning_rate': 0.21786312525475518, 'max_depth': 2, 'subsample': 0.6858099646533395, 'colsample_bytree': 0.9629939240820189, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:34:52,104] Trial 85 finished with value: 0.7704206128974658 and parameters: {'regressor': 'GradientBoostingRegressor', 'n_estimators': 108, 'subsample': 0.7173946106942692, 'learning_rate': 0.2927755347390036, 'min_samples_split': 10, 'max_depth': 3, 'max_features': 'sqrt', 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:12,725] Trial 86 finished with value: 0.6873070146995488 and parameters: {'regressor': 'AdaBoostRegressor', 'n_estimators': 126, 'learning_rate': 0.2868970386293195, 'loss': 'linear', 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:16,833] Trial 87 finished with value: 0.763233879825719 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 62, 'learning_rate': 0.11423114236454876, 'max_depth': 3, 'subsample': 0.7394202514598981, 'colsample_bytree': 0.9948411318577763, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:27,867] Trial 88 finished with value: 0.7636373748937899 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 173, 'learning_rate': 0.1600975298689179, 'max_depth': 5, 'subsample': 0.7619362503780621, 'colsample_bytree': 0.786234765035458, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:38,449] Trial 89 finished with value: 0.7784482075822915 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 164, 'learning_rate': 0.11869310683590627, 'max_depth': 5, 'subsample': 0.665308149864482, 'colsample_bytree': 0.952064629617783, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:48,494] Trial 90 finished with value: 0.7799352084830704 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 192, 'learning_rate': 0.21215626429482334, 'max_depth': 3, 'subsample': 0.7037339788537716, 'colsample_bytree': 0.9733043710327632, 'target_transformer': 'sqrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:38:58,440] Trial 91 finished with value: 0.7837697889088012 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 142, 'learning_rate': 0.13817874615905554, 'max_depth': 4, 'subsample': 0.6955197840789762, 'colsample_bytree': 0.9969295623822237, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:07,804] Trial 92 finished with value: 0.7927818782447426 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 147, 'learning_rate': 0.09423093169068199, 'max_depth': 4, 'subsample': 0.6553542388630191, 'colsample_bytree': 0.9818656109859603, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:14,034] Trial 93 finished with value: 0.7823666980573765 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 80, 'learning_rate': 0.09637609334623806, 'max_depth': 4, 'subsample': 0.6606863409287265, 'colsample_bytree': 0.5239735397179909, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:24,693] Trial 94 finished with value: 0.7908388688358432 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 178, 'learning_rate': 0.05643701003074245, 'max_depth': 4, 'subsample': 0.6398794037471737, 'colsample_bytree': 0.9349640389809181, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:25,680] Trial 95 finished with value: 0.6944807968665189 and parameters: {'regressor': 'LinearRegression', 'fit_intercept': True, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:37,444] Trial 96 finished with value: 0.7917112365652906 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 183, 'learning_rate': 0.05545404269856546, 'max_depth': 5, 'subsample': 0.5226785323063979, 'colsample_bytree': 0.928652858678777, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:39:44,244] Trial 97 finished with value: -0.059271810229728716 and parameters: {'regressor': 'LassoRegression', 'alpha': 0.08578357605615321, 'fit_intercept': False, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:42:02,124] Trial 98 finished with value: 0.7162133108442239 and parameters: {'regressor': 'RandomForestRegressor', 'n_estimators': 182, 'max_depth': 15, 'min_samples_split': 5, 'max_features': None, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n",
      "[I 2025-08-11 11:42:11,854] Trial 99 finished with value: 0.786717690894934 and parameters: {'regressor': 'XGBRegressor', 'n_estimators': 195, 'learning_rate': 0.05537668619263989, 'max_depth': 5, 'subsample': 0.6435379146811326, 'colsample_bytree': 0.9741681371902309, 'target_transformer': 'cbrt'}. Best is trial 81 with value: 0.7957138059273478.\n"
     ]
    }
   ],
   "source": [
    "study =optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective,n_trials = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "45b9aa9d-a23a-4b2b-a74e-ee96347824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are : {'regressor': 'XGBRegressor', 'n_estimators': 153, 'learning_rate': 0.12744409833293713, 'max_depth': 4, 'subsample': 0.687906288849064, 'colsample_bytree': 0.9966478096303656, 'target_transformer': 'cbrt'} and the best value obtained is 0.7957138059273478\n"
     ]
    }
   ],
   "source": [
    "print(f'Best params are : {study.best_params} and the best value obtained is {study.best_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "177acde7-ae13-4f68-8eb5-75896980f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_scores for train and val are 0.8219857533156929 and 0.8088497796215204\n"
     ]
    }
   ],
   "source": [
    "regressor_name = study.best_params[\"regressor\"]\n",
    "transformer_name = study.best_params[\"target_transformer\"]\n",
    "\n",
    "regressor_dict = {\n",
    "    \"LinearRegression\": LinearRegression,\n",
    "    \"RidgeRegression\": Ridge,\n",
    "    \"LassoRegression\": Lasso,\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor,\n",
    "    \"RandomForestRegressor\": RandomForestRegressor,\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor,\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor,\n",
    "    \"XGBRegressor\": XGBRegressor\n",
    "}\n",
    "\n",
    "transformer_dict = {\n",
    "    \"log\": FunctionTransformer(\n",
    "            func = np.log,\n",
    "            inverse_func = np.exp\n",
    "        ),\n",
    "    \"sqrt\": FunctionTransformer(\n",
    "            func = np.sqrt,\n",
    "            inverse_func = lambda x:x**2\n",
    "        ),\n",
    "    \"cbrt\": FunctionTransformer(\n",
    "            func = np.cbrt,\n",
    "            inverse_func = lambda x:x**3\n",
    "        ),\n",
    "    \"same\": FunctionTransformer(\n",
    "            func = lambda x:x,\n",
    "            inverse_func = lambda x:x\n",
    "        )\n",
    "}\n",
    "\n",
    "params_for_model = {k: v for k, v in study.best_params.items() if k !=\"regressor\" and k!=\"transformer\"}\n",
    "\n",
    "regressor = regressor_dict[regressor_name](**params_for_model)\n",
    "transformer = transformer_dict[transformer_name]\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "        (\"model\", TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=transformer\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "model_pipeline.fit(X_train_preprocessed,y_train)\n",
    "y_train_pred = model_pipeline.predict(X_train_preprocessed)\n",
    "y_pred = model_pipeline.predict(X_val_preprocessed)\n",
    "\n",
    "print(f'r2_scores for train and val are {r2_score(y_train,y_train_pred)} and {r2_score(y_val,y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
